{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOPEcdIO0oIyCUt9Mux5sfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wrobbins0409/cse30124-project/blob/main/introToAI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro to AI Project - Classical Sentiment Analysis vs Machine learning ###\n",
        "\n"
      ],
      "metadata": {
        "id": "C0j7pzp_nGMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I have attempted to fine tune an existing LLM for the purpose of sentiment analysis in order to learn the process of doing so and also to test on a dataset whether traditional sentiment analysis or machine learning based sentiment analysis works better.\n",
        "\n",
        "I started with the model head from the distilbert-base-uncased model as this is optimized for downstream finetuning for a variety of language tasks. I used the glue dataset to finetune the model utilizing the sst2 portion of the dataset which includes sentence data that is labeled with either a 0 or 1 corresponding to negative or positive sentiment respectively. Here is a link to the dataset [glue/sst2](https://huggingface.co/datasets/glue/viewer/sst2)\n",
        "\n",
        "\n",
        "\n",
        "In the repo the python script used to train the model is included but in this notebook we will be demonstrating its use and comparing it with traditional methods of sentiment analysis."
      ],
      "metadata": {
        "id": "-p-k931I6mGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and installations\n",
        "# !pip install datasets\n",
        "# !pip install transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from datasets import load_dataset, load_metric\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "kGVglgRp8yL1"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start by loading in the model into a pipeline using transformers which makes it very easy to plug and play with any public model on HuggingFace"
      ],
      "metadata": {
        "id": "li0MO4tr8YJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load model into pipeline\n",
        "pipe = pipeline(\"sentiment-analysis\", model=\"wrobbins0409/distilbert-base-uncased-finetuned-sst2-wrobbins\", truncation=True, max_length=512)\n",
        "\n",
        "# test pipeline on negative sentence that should return a negative label\n",
        "pipe(\"I hate everything!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04-QCmlw6kV6",
        "outputId": "30ecedaa-4ef3-45ff-da90-3748d30fe4f0"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'negative', 'score': 0.9999563694000244}]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline returns a list with a dict that has the label of the classified text and the associated score of the label which tends to be extreme towards either negative or positive."
      ],
      "metadata": {
        "id": "_2i77rO2GpHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model on a dataset ###\n",
        "\n",
        "Lets see how this model does in predicting labels on a 2 label dataset with labels of either positive or negative and compare its accuracy to that of the VADER method for text classification. We will be testing on the validation set of the glue dataset because the test set has invalid labels."
      ],
      "metadata": {
        "id": "ZzeaySpWG-ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load a dataset with only the testing data\n",
        "dataset = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
        "\n",
        "# look at contents of dataset, in this case it will be\n",
        "print(dataset)\n",
        "\n",
        "# map label to name\n",
        "labels = {\n",
        "    0: 'negative',\n",
        "    1: 'positive'\n",
        "}\n",
        "\n",
        "\n",
        "# show examples of the data\n",
        "for i, entry in enumerate(dataset):\n",
        "\n",
        "    # stop after ten examples\n",
        "    if i == 10:\n",
        "        break\n",
        "\n",
        "    # print out the text data and the coresponding label (limiting text to 200 chars for readability)\n",
        "    print(f'Sentence: {entry[\"sentence\"][:200]}, Label: {labels[entry[\"label\"]]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9uIDMssUhOd",
        "outputId": "b4d3e4bc-110c-43fa-f9db-d9f105d78017"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['sentence', 'label', 'idx'],\n",
            "    num_rows: 872\n",
            "})\n",
            "Sentence: it 's a charming and often affecting journey . , Label: positive\n",
            "Sentence: unflinchingly bleak and desperate , Label: negative\n",
            "Sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . , Label: positive\n",
            "Sentence: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . , Label: positive\n",
            "Sentence: it 's slow -- very , very slow . , Label: negative\n",
            "Sentence: although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . , Label: positive\n",
            "Sentence: a sometimes tedious film . , Label: negative\n",
            "Sentence: or doing last year 's taxes with your ex-wife . , Label: negative\n",
            "Sentence: you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . , Label: positive\n",
            "Sentence: in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . , Label: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will need to set up the VADER model for sentiment analysis. VADER operates by assigning a polarity (positive, negative, or neutral) to each word in a given text and then combining these individual polarities to calculate an overall sentiment score for the entire text. It does not use any machine learning so it will be interesting to see if this older Lexical model still outperforms the Machine Learning approcah"
      ],
      "metadata": {
        "id": "i-_hQ9B8E0vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# define function to return sentiment score based on\n",
        "def calculate_vader_sentiment(text, return_scores = False):\n",
        "    scores = sia.polarity_scores(text)\n",
        "    compound_score = scores['compound']\n",
        "    if return_scores:\n",
        "        return scores\n",
        "    else:\n",
        "        return 1 if compound_score >= 0 else 0\n",
        "\n",
        "# define sentence\n",
        "sentence = \"I hate everything!\"\n",
        "\n",
        "# test function with compound and total scores, negative sentiment = 0, positive = 1\n",
        "print(f'Sentence: {sentence}, Scores: {calculate_vader_sentiment(sentence, return_scores = True)}, Sentiment: {labels[calculate_vader_sentiment(sentence)]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwdAWYhTE43W",
        "outputId": "63aff8b4-bb92-4c89-ce63-3d03b90834f2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: I hate everything!, Scores: {'neg': 0.8, 'neu': 0.2, 'pos': 0.0, 'compound': -0.6114}, Sentiment: negative\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are sure that both of our models are working, we need to test them on the dataset to see which one performs better."
      ],
      "metadata": {
        "id": "Bf4jVQG3Ifxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make preprocessing function for\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_vader_predictions = 0\n",
        "correct_mymodel_predictions = 0\n",
        "examples = 0\n",
        "\n",
        "for example in tqdm(dataset):\n",
        "    examples += 1\n",
        "    text = example['sentence']\n",
        "    label = int(example['label'])\n",
        "\n",
        "    # Calculate VADER sentiment\n",
        "    vader_sentiment = calculate_vader_sentiment(text)\n",
        "    if vader_sentiment == label:\n",
        "        correct_vader_predictions += 1\n",
        "\n",
        "    # Calculate Mymodel prediction\n",
        "    mymodel_pipeline_result = pipe(text)\n",
        "    if mymodel_pipeline_result[0]['label'] == 'negative':\n",
        "        mymodel_prediction = 0\n",
        "    else:\n",
        "        mymodel_prediction = 1\n",
        "\n",
        "    if mymodel_prediction == label:\n",
        "        correct_mymodel_predictions += 1\n",
        "\n",
        "accuracy_vader = float(correct_vader_predictions / examples)\n",
        "accuracy_mymodel = float(correct_mymodel_predictions / examples)\n",
        "\n",
        "print(f'\\nvader accuracy: {accuracy_vader}, my models accuracy: {accuracy_mymodel}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYxJ6cVLJ5p3",
        "outputId": "d5441e13-a9bc-425c-faf9-0d7ca6861d5b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 872/872 [00:58<00:00, 14.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "vader accuracy: 0.625, my models accuracy: 0.9059633027522935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's pretty apparent from this testing that the my finetuned model is significantly better at correctly classifying text. Mine achieves an accuracy of 90.6% and the vader model only achieves 62.5% accuracy. However, I am curious to see how my model matches up against another sentiment analysis model trained on similar data with the same model-head, that being the distilbert-base-uncased. For this testing I will be comparing my model with the HuggingFace model distilbert-base-uncased-finetuned-sst-2-english which also uses the sst2 set for finetuning."
      ],
      "metadata": {
        "id": "CD5rLs25R6SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load pipeline for new model\n",
        "hf_pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", truncation=True, max_length=512)\n",
        "\n",
        "# test model with sample input\n",
        "hf_pipe(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeTIOwvQRquZ",
        "outputId": "365ed5b1-519d-4e37-e45d-99644016c033"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9992390871047974}]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that it appears to be working lets see how my model matches up against this new one"
      ],
      "metadata": {
        "id": "L6F4-8shYiP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "# Calculate accuracy\n",
        "correct_hf_predictions = 0\n",
        "correct_mymodel_predictions = 0\n",
        "examples = 0\n",
        "\n",
        "for example in tqdm(dataset):\n",
        "    examples += 1\n",
        "    text = example['sentence']\n",
        "    label = int(example['label'])\n",
        "\n",
        "    # Calculate HF sentiment\n",
        "    hf_pipeline_result = hf_pipe(text)\n",
        "    if hf_pipeline_result[0]['label'] == 'NEGATIVE':\n",
        "        hf_prediction = 0\n",
        "    else:\n",
        "        hf_prediction = 1\n",
        "\n",
        "    if hf_prediction == label:\n",
        "        correct_hf_predictions += 1\n",
        "\n",
        "    # Calculate Mymodel prediction\n",
        "    mymodel_pipeline_result = pipe(text)\n",
        "    if mymodel_pipeline_result[0]['label'] == 'negative':\n",
        "        mymodel_prediction = 0\n",
        "    else:\n",
        "        mymodel_prediction = 1\n",
        "\n",
        "    if mymodel_prediction == label:\n",
        "        correct_mymodel_predictions += 1\n",
        "\n",
        "accuracy_hf = float(correct_hf_predictions / examples)\n",
        "accuracy_mymodel = float(correct_mymodel_predictions / examples)\n",
        "\n",
        "print(f'\\nHugging Face accuracy: {accuracy_hf}, my models accuracy: {accuracy_mymodel}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF0NKHJnYn-2",
        "outputId": "78c17ae1-5f22-4811-9fd2-c5995eb67e1a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 872/872 [01:59<00:00,  7.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hugging Face accuracy: 0.9105504587155964, my models accuracy: 0.9059633027522935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With my model achieving 90.6% accuracy, I would say that it matched up quite well against the HuggingFace model which achieved an accuracy of 91.1%. There could be multiple reasons for this such as the learning rate or batch size. I was limited in my batch size to 8 due to the fact that anything larger would cause my computer to blue screen. Perhaps in the future with more hardware I could utilize larger batch sizes in order to achieve better accuracy.\n",
        "\n",
        "Overall, I learned a lot in finetuning this model. I initially attempted to train one from scratch but quickly learned it would require more resources and time than what was reasonable so instead went the finetuning route. Still, it was quite a challenge that required a lot of research in trying to find the best tools for the job and also the best datasets while keeping the scope manageable for a laptop with just a mobile GPU.\n",
        "\n",
        "I learned a lot and it was enjoyable, my model is also available on hugging face for use for sentiment analysis at this [link](https://huggingface.co/wrobbins0409/distilbert-base-uncased-finetuned-sst2-wrobbins)."
      ],
      "metadata": {
        "id": "2-KntiGDd-GJ"
      }
    }
  ]
}